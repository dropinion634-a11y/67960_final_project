<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <title>Pruning LLMs with Reward Model Gradients</title>

    <style>
        /* ----- Global styles ----- */
        :root {
            /* Body text: SF Pro Text on macOS, otherwise system UI */
            --body-font: -apple-system, BlinkMacSystemFont, "SF Pro Text",
                system-ui, "Segoe UI", sans-serif;

            /* Title / monospaced text: SF Mono on macOS, otherwise common monospace */
            --mono-font: "SF Mono", SFMono-Regular, ui-monospace,
                Menlo, Monaco, Consolas, "Liberation Mono",
                "Courier New", monospace;
        }

        * {
            box-sizing: border-box;
        }

        html {
            scroll-behavior: smooth;
        }

        body {
            margin: 0;
            padding: 0;
            background-color: #f3f7fd;
            color: #111;
            font-family: var(--body-font);
            font-size: 17px;
            /* closer to original body size */
            line-height: 1.55;
            /* slightly tighter like screenshot */
            -webkit-font-smoothing: antialiased;
            -moz-osx-font-smoothing: grayscale;
        }

        a {
            color: #00796b;
            text-decoration: none;
        }

        a:hover {
            text-decoration: underline;
        }

        /* ----- Sidebar ----- */
        .sidebar {
            position: fixed;
            top: 120px;
            left: 40px;
            width: 170px;
            padding-left: 4px;
            font-size: 14px;
        }

        .sidebar-title {
            font-weight: 700;
            margin-bottom: 12px;
        }

        .sidebar-nav {
            list-style: none;
            padding: 0;
            margin: 0;
        }

        .sidebar-nav li {
            margin-bottom: 14px;
        }

        .sidebar-nav a {
            display: inline-block;
            line-height: 1.3;
        }

        /* ----- Main paper layout ----- */
        .main {
            margin-left: 240px;
            /* leave space for sidebar */
            padding: 24px 32px 60px 0;
        }

        .paper {
            background-color: #ffffff;
            border: 1px solid #e1e4ec;
            margin: 8px 40px 40px;
            padding: 24px 32px 36px;
            max-width: 980px;
        }

        /* ----- Title block ----- */
        .paper-title {
            font-family: var(--mono-font);
            /* SF Mono-style title */
            font-size: 2.5rem;
            /* ~40px */
            font-weight: 400;
            line-height: 1.25;
            margin: 0 0 20px 0;
            letter-spacing: 0.06em;
            /* a bit more spacing like original */
        }

        .paper-meta {
            font-size: 15px;
            margin-bottom: 18px;
        }

        .paper-divider {
            height: 1px;
            background-color: #e7ebf5;
            margin: 12px -32px 24px;
        }

        /* ----- Sections ----- */
        section {
            margin-bottom: 32px;
        }

        section h2 {
            font-size: 22px;
            /* closer to original heading size */
            margin: 0 0 10px 0;
            font-weight: 700;
        }

        p {
            margin: 0 0 14px 0;
            /* a bit more space between paragraphs */
        }

        /* ----- Citations (inline) ----- */
        .cite {
            font-size: 0.75em;
            vertical-align: super;
        }

        /* ----- References list ----- */
        .references {
            font-size: 15px;
            margin-top: 8px;
        }

        .references ol {
            list-style: none;
            /* remove default 1. 2. 3. */
            counter-reset: ref-counter;
            padding-left: 0;
            margin: 0;
        }

        .references li {
            counter-increment: ref-counter;
            margin-bottom: 8px;
        }

        .references li::before {
            content: "[" counter(ref-counter) "] ";
        }

        /* ----- Figures with right-side caption ----- */
        .figure-row {
            display: flex;
            align-items: flex-start;
            margin: 24px 0 32px 0;
        }

        .figure-main {
            flex: 1 1 auto;
        }

        .figure-main img {
            max-width: 100%;
            height: auto;
            display: block;
        }

        .figure-caption {
            flex: 0 0 230px;
            margin-left: 24px;
            font-size: 14px;
            line-height: 1.4;
            color: #444;
        }

        @media (max-width: 900px) {
            .figure-row {
                flex-direction: column;
            }

            .figure-caption {
                margin-left: 0;
                margin-top: 12px;
            }
        }
    </style>

    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>

    <!-- Fixed Outline sidebar -->
    <aside class="sidebar">
        <div class="sidebar-title">Outline</div>
        <ul class="sidebar-nav">
            <li><a href="#introduction">Introduction</a></li>
            <li><a href="#background">Background and<br>Related Work</a></li>
            <li><a href="#methods">Methods</a></li>
            <li><a href="#results">Results</a></li>
            <li><a href="#conclusion">Conclusion</a></li>
            <li><a href="#references">References</a></li>
        </ul>
    </aside>

    <!-- Main document -->
    <div class="main">
        <article class="paper">
            <!-- Title & author info -->
            <header>
                <h1 class="paper-title">
                    Pruning LLMs with Reward Model Gradients
                </h1>
                <div class="paper-meta">
                    Joshua Bello, Diego Coello, and Jabes Gallardo<br>
                    Final project for 6.7960, MIT
                </div>
                <div class="paper-divider"></div>
            </header>

            <!-- Sections -->
            <section id="introduction">
                <h2>Introduction</h2>
                <p>
                    Model compression is the task of reducing the size and computational demand of a machine learning
                    model while maintaining performance comparable to the uncompressed (baseline) model on a set of
                    task-dependent benchmarks. Although compression can be applied to any neural network architecture,
                    our work focuses on large language models (LLMs) because they show tremendous natural language
                    processing capabilities but are computationally intensive.
                </p>

                <p>
                    The compression technique that we focus on is pruning: systematically removing individual weights,
                    neurons, or entire layers from a neural network by zeroing out their values based on some importance
                    criterion. One reason for choosing pruning as the focus of our study is that it offers memory usage
                    and latency reduction, which matters for independent AI developers who rely on locally-hosted
                    models. Even companies like OpenAI seek to reduce their model usage costs, since they pay for the
                    GPU/TPU compute time required for inference whenever one of their hundreds of millions of users
                    queries their models. Pruning also acts as a post-hoc regularizer for downstream tasks by removing
                    redundant weights and decreasing potential overfitting in pre-trained LLMs.
                </p>

                <p>
                    We consider pruning methods from the existing literature, some of which eliminate a certain
                    percentage of weights based on their magnitude and the activation of the previous layer’s outputs.
                    In our research, we took inspiration from recent work<span class="cite">[1]</span>, which developed
                    a pruning
                    technique that uses the gradient of the loss function with respect to the weights as part of the
                    pruning metric. Based on this, we train separate reward models to use in our gradient computations
                    and compare performance against existing techniques. We use reward models to mimic human feedback
                    according to different language metrics.
                </p>

                <!-- Example figure block; replace src and caption when you have a real figure
                <div class="figure-row">
                    <div class="figure-main">
                        <img src="figure1.png" alt="Example plot of pruning performance">
                    </div>
                    <div class="figure-caption">
                        <strong>Figure 1:</strong> Example caption describing the main result of the figure.
                        Update this text and the image source once you have your actual figure.
                    </div>
                </div>
                -->
            </section>

            <section id="background">
                <h2>Background and Related Work</h2>
                <p>
                    <!-- Your background text goes here -->
                    Curabitur ligula sapien, tincidunt non, euismod vitae, posuere
                    imperdiet, leo. Maecenas malesuada.
                </p>
                <p>
                    \[
                    \mathbf{W}_m[i,j] = \alpha \cdot \left|\mathbf{W}[i,j]\right| \cdot \left\|\mathbf{G}[:,
                    i,j]\right\|_{p}
                    + \left|\mathbf{W}[i,j]\right| \cdot \left\| \mathbf{X}[:,j]\right\|_2
                    \]
                </p>
            </section>

            <section id="methods">
                <h2>Methods</h2>
                <p>
                    <!-- Your methods text goes here -->
                    Fusce fermentum odio nec arcu. Vivamus viverra fermentum felis.
                </p>
            </section>

            <section id="results">
                <h2>Results</h2>
                <p>
                    <!-- Your results text goes here -->
                    Pellentesque habitant morbi tristique senectus et netus et malesuada
                    fames ac turpis egestas.
                </p>

                <!-- 7 tasks also used in WANDA and GLP Pruner paper -->
                <table border="1" cellspacing="0" cellpadding="4">
                    <thead>
                        <tr>
                            <th></th>
                            <th>boolq</th>
                            <th>rte</th>
                            <th>hellaswag</th>
                            <th>arc_easy</th>
                            <th>arc_challenge</th>
                            <th>winogrande</th>
                            <th>openbookqa</th>
                            <th>mean</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>No Pruning</td>
                            <td>0.64</td>
                            <td>0.56</td>
                            <td>0.48</td>
                            <td>0.66</td>
                            <td>0.32</td>
                            <td>0.61</td>
                            <td>0.28</td>
                            <td>0.51</td>
                        </tr>
                        <tr>
                            <td>Arena (50%)</td>
                            <td>0.61</td>
                            <td>0.52</td>
                            <td>0.35</td>
                            <td>0.53</td>
                            <td>0.23</td>
                            <td>0.55</td>
                            <td>0.17</td>
                            <td>0.42</td>
                        </tr>
                        <tr>
                            <td>HH (50%)</td>
                            <td>0.61</td>
                            <td>0.53</td>
                            <td>0.30</td>
                            <td>0.42</td>
                            <td>0.20</td>
                            <td>0.52</td>
                            <td>0.12</td>
                            <td>0.39</td>
                        </tr>
                        <tr>
                            <td>Magnitude (50%)</td>
                            <td>0.39</td>
                            <td>0.50</td>
                            <td>0.26</td>
                            <td>0.27</td>
                            <td>0.19</td>
                            <td>0.49</td>
                            <td>0.15</td>
                            <td>0.32</td>
                        </tr>
                        <tr>
                            <td>UF (50%)</td>
                            <td>0.59</td>
                            <td>0.53</td>
                            <td>0.29</td>
                            <td>0.40</td>
                            <td>0.19</td>
                            <td>0.51</td>
                            <td>0.12</td>
                            <td>0.38</td>
                        </tr>
                    </tbody>
                </table>

                <table border="1" cellspacing="0" cellpadding="4">
                    <thead>
                        <tr>
                            <th></th>
                            <th>No Pruning</th>
                            <th>Magnitude (50%)</th>
                            <th>HH (50%)</th>
                            <th>UF (50%)</th>
                            <th>Arena (50%)</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Perplexity</td>
                            <td>9.916</td>
                            <td>1732.262</td>
                            <td>54.618</td>
                            <td>61.442</td>
                            <td>22.561</td>
                        </tr>
                    </tbody>
                </table>

            </section>

            <section id="conclusion">
                <h2>Conclusion</h2>
                <p>
                    <!-- Your conclusion text goes here -->
                    Integer lacinia. Praesent blandit laoreet nibh. Fusce convallis
                    metus id felis luctus adipiscing.
                </p>
            </section>

            <section id="references">
                <h2>References</h2>
                <div class="references">
                    <ol>
                        <li id="ref-sun2023-simple">
                            M. Sun, Z. Liu, A. Bair, and J. Z. Kolter, “A Simple and Effective Pruning Approach for
                            Large Language Models,” <em>arXiv preprint</em> arXiv:2306.11695, 2023. [Online]. Available:
                            <a href="https://arxiv.org/abs/2306.11695" target="_blank" rel="noopener noreferrer">
                                https://arxiv.org/abs/2306.11695
                            </a>
                        </li>
                        <li id="ref-das2023-beyond">
                            R. J. Das, M. Sun, L. Ma, and Z. Shen, “Beyond Size: How Gradients Shape Pruning Decisions
                            in Large Language Models,” <em>arXiv preprint</em> arXiv:2311.04902, 2023. [Online].
                            Available:
                            <a href="https://arxiv.org/abs/2311.04902" target="_blank" rel="noopener noreferrer">
                                https://arxiv.org/abs/2311.04902
                            </a>
                        </li>
                        <li id="ref-ouyang2022-instructgpt">
                            L. Ouyang, J. Wu, X. Jiang, <em>et al.</em>, “Training Language Models to Follow
                            Instructions with Human Feedback,” <em>arXiv preprint</em> arXiv:2203.02155, 2022.
                            [Online]. Available:
                            <a href="https://arxiv.org/abs/2203.02155" target="_blank" rel="noopener noreferrer">
                                https://arxiv.org/abs/2203.02155
                            </a>
                        </li>
                        <li id="ref-lee2018-snip">
                            N. Lee, T. Ajanthan, and P. H. S. Torr, “SNIP: Single-shot Network Pruning Based on
                            Connection Sensitivity,” <em>arXiv preprint</em> arXiv:1810.02340, 2018. [Online].
                            Available:
                            <a href="https://arxiv.org/abs/1810.02340" target="_blank" rel="noopener noreferrer">
                                https://arxiv.org/abs/1810.02340
                            </a>
                        </li>

                        <!-- New references from the WhatsApp message -->

                        <!-- lm-evaluation-harness (Gao et al., 2021) -->
                        <li id="ref-gao2021-lmeval">
                            L. Gao, J. Tow, S. Biderman, S. Black, A. DiPofi, C. Foster, <em>et al.</em>, “A Framework
                            for
                            Few-shot Language Model Evaluation,” 2021. [Online]. Available:
                            <a href="https://github.com/EleutherAI/lm-evaluation-harness" target="_blank"
                                rel="noopener noreferrer">
                                https://github.com/EleutherAI/lm-evaluation-harness
                            </a>
                        </li>

                        <!-- BoolQ (Clark et al., 2019) -->
                        <li id="ref-clark2019-boolq">
                            C. Clark, K. Lee, M.-W. Chang, T. Kwiatkowski, M. Collins, and K. Toutanova,
                            “BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions,” in
                            <em>Proc. NAACL-HLT</em>, 2019. [Online]. Available:
                            <a href="https://arxiv.org/abs/1905.10044" target="_blank" rel="noopener noreferrer">
                                https://arxiv.org/abs/1905.10044
                            </a>
                        </li>

                        <!-- RTE / GLUE (Wang et al., 2018) -->
                        <li id="ref-wang2018-glue">
                            A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman,
                            “GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding,”
                            <em>arXiv preprint</em> arXiv:1804.07461, 2018. [Online]. Available:
                            <a href="https://arxiv.org/abs/1804.07461" target="_blank" rel="noopener noreferrer">
                                https://arxiv.org/abs/1804.07461
                            </a>
                        </li>

                        <!-- HellaSwag (Zellers et al., 2019) -->
                        <li id="ref-zellers2019-hellaswag">
                            R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi,
                            “HellaSwag: Can a Machine Really Finish Your Sentence?,” in
                            <em>Proc. ACL</em>, 2019. [Online]. Available:
                            <a href="https://arxiv.org/abs/1905.07830" target="_blank" rel="noopener noreferrer">
                                https://arxiv.org/abs/1905.07830
                            </a>
                        </li>

                        <!-- WinoGrande (Sakaguchi et al., 2019) -->
                        <li id="ref-sakaguchi2019-winogrande">
                            K. Sakaguchi, R. Bras, C. Bhagavatula, and Y. Choi,
                            “WinoGrande: An Adversarial Winograd Schema Challenge at Scale,”
                            <em>arXiv preprint</em> arXiv:1907.10641, 2019. [Online]. Available:
                            <a href="https://arxiv.org/abs/1907.10641" target="_blank" rel="noopener noreferrer">
                                https://arxiv.org/abs/1907.10641
                            </a>
                        </li>

                        <!-- ARC-easy / ARC-challenge (Clark et al., 2018) -->
                        <li id="ref-clark2018-arc">
                            P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord,
                            “Think You Have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge,”
                            <em>arXiv preprint</em> arXiv:1803.05457, 2018. [Online]. Available:
                            <a href="https://arxiv.org/abs/1803.05457" target="_blank" rel="noopener noreferrer">
                                https://arxiv.org/abs/1803.05457
                            </a>
                        </li>

                        <!-- OBQA (Mihaylov et al., 2018) -->
                        <li id="ref-mihaylov2018-openbookqa">
                            T. Mihaylov, P. Clark, T. Khot, and A. Sabharwal,
                            “Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering,”
                            in <em>Proc. EMNLP</em>, 2018. [Online]. Available:
                            <a href="https://arxiv.org/abs/1809.02789" target="_blank" rel="noopener noreferrer">
                                https://arxiv.org/abs/1809.02789
                            </a>
                        </li>

                        <!-- Dettmers & Zettlemoyer, 2022 (LLM.int8) -->
                        <li id="ref-dettmers2022-llmint8">
                            T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer,
                            “LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale,”
                            <em>arXiv preprint</em> arXiv:2208.07339, 2022. [Online]. Available:
                            <a href="https://arxiv.org/abs/2208.07339" target="_blank" rel="noopener noreferrer">
                                https://arxiv.org/abs/2208.07339
                            </a>
                        </li>

                        <!-- Frantar & Alistarh, 2023 (SparseGPT) -->
                        <li id="ref-frantar2023-sparsegpt">
                            E. Frantar and D. Alistarh,
                            “SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot,”
                            in <em>Proc. ICML</em>, 2023. [Online]. Available:
                            <a href="https://arxiv.org/abs/2301.00774" target="_blank" rel="noopener noreferrer">
                                https://arxiv.org/abs/2301.00774
                            </a>
                        </li>
                    </ol>
                </div>
            </section>

        </article>
    </div>

</body>

</html>