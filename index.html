<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <title>Pruning LLMs with Reward Model Gradients</title>

    <style>
        /* ----- Global styles ----- */
        :root {
            /* Body text: SF Pro Text on macOS, otherwise system UI */
            --body-font: -apple-system, BlinkMacSystemFont, "SF Pro Text",
                system-ui, "Segoe UI", sans-serif;

            /* Title / monospaced text: SF Mono on macOS, otherwise common monospace */
            --mono-font: "SF Mono", SFMono-Regular, ui-monospace,
                Menlo, Monaco, Consolas, "Liberation Mono",
                "Courier New", monospace;
        }

        * {
            box-sizing: border-box;
        }

        html {
            scroll-behavior: smooth;
        }

        body {
            margin: 0;
            padding: 0;
            background-color: #f3f7fd;
            color: #111;
            font-family: var(--body-font);
            font-size: 17px;
            /* closer to original body size */
            line-height: 1.55;
            /* slightly tighter like screenshot */
            -webkit-font-smoothing: antialiased;
            -moz-osx-font-smoothing: grayscale;
        }

        a {
            color: #00796b;
            text-decoration: none;
        }

        a:hover {
            text-decoration: underline;
        }

        /* ----- Sidebar ----- */
        .sidebar {
            position: fixed;
            top: 120px;
            left: 40px;
            width: 170px;
            padding-left: 4px;
            font-size: 14px;
        }

        .sidebar-title {
            font-weight: 700;
            margin-bottom: 12px;
        }

        .sidebar-nav {
            list-style: none;
            padding: 0;
            margin: 0;
        }

        .sidebar-nav li {
            margin-bottom: 14px;
        }

        .sidebar-nav a {
            display: inline-block;
            line-height: 1.3;
        }

        /* ----- Main paper layout ----- */
        .main {
            margin-left: 240px;
            /* leave space for sidebar */
            padding: 24px 32px 60px 0;
        }

        .paper {
            background-color: #ffffff;
            border: 1px solid #e1e4ec;
            margin: 8px 40px 40px;
            padding: 24px 32px 36px;
            max-width: 980px;
        }

        /* ----- Title block ----- */
        .paper-title {
            font-family: var(--mono-font);
            /* SF Mono-style title */
            font-size: 2.5rem;
            /* ~40px */
            font-weight: 400;
            line-height: 1.25;
            margin: 0 0 20px 0;
            letter-spacing: 0.06em;
            /* a bit more spacing like original */
        }

        .paper-meta {
            font-size: 15px;
            margin-bottom: 18px;
        }

        .paper-divider {
            height: 1px;
            background-color: #e7ebf5;
            margin: 12px -32px 24px;
        }

        /* ----- Sections ----- */
        section {
            margin-bottom: 32px;
        }

        section h2 {
            font-size: 22px;
            /* closer to original heading size */
            margin: 0 0 10px 0;
            font-weight: 700;
        }

        p {
            margin: 0 0 14px 0;
            /* a bit more space between paragraphs */
        }

        /* ----- Citations (inline) ----- */
        .cite {
            font-size: 0.75em;
            vertical-align: super;
        }

        /* ----- References list ----- */
        .references {
            font-size: 15px;
            margin-top: 8px;
        }

        .references ol {
            list-style: none;
            /* remove default 1. 2. 3. */
            counter-reset: ref-counter;
            padding-left: 0;
            margin: 0;
        }

        .references li {
            counter-increment: ref-counter;
            margin-bottom: 8px;
        }

        .references li::before {
            content: "[" counter(ref-counter) "] ";
        }

        /* ----- Figures with right-side caption ----- */
        .figure-row {
            display: flex;
            align-items: flex-start;
            margin: 24px 0 32px 0;
        }

        .figure-main {
            flex: 1 1 auto;
        }

        .figure-main img {
            max-width: 100%;
            height: auto;
            display: block;
        }

        .figure-caption {
            flex: 0 0 230px;
            margin-left: 24px;
            font-size: 14px;
            line-height: 1.4;
            color: #444;
        }

        @media (max-width: 900px) {
            .figure-row {
                flex-direction: column;
            }

            .figure-caption {
                margin-left: 0;
                margin-top: 12px;
            }
        }
    </style>

    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>

    <!-- Fixed Outline sidebar -->
    <aside class="sidebar">
        <div class="sidebar-title">Outline</div>
        <ul class="sidebar-nav">
            <li><a href="#introduction">Introduction</a></li>
            <li><a href="#background">Background and<br>Related Work</a></li>
            <li><a href="#methods">Methods</a></li>
            <li><a href="#results">Results</a></li>
            <li><a href="#conclusion">Conclusion</a></li>
            <li><a href="#references">References</a></li>
        </ul>
    </aside>

    <!-- Main document -->
    <div class="main">
        <article class="paper">
            <!-- Title & author info -->
            <header>
                <h1 class="paper-title">
                    Pruning LLMs with Reward Model Gradients
                </h1>
                <div class="paper-meta">
                    Joshua Bello, Diego Coello, and Jabes Gallardo<br>
                    Final project for 6.7960, MIT
                </div>
                <div class="paper-divider"></div>
            </header>

            <!-- Sections -->
            <section id="introduction">
                <h2>Introduction</h2>

                <p>Modern large language models (LLMs) achieve strong performance largely due to their scale, but this
                    scale makes them expensive. While very large models are often necessary during training, using them
                    directly at inference time incurs high costs in memory, latency, energy, and hardware. These
                    constraints limit real-world deployment, especially for edge devices, low-resource settings, and
                    independent developers. Model compression addresses this training–deployment mismatch by enabling
                    smaller, more efficient inference models while preserving performance close to the uncompressed
                    baseline.</p>

                <p>Common approaches to model compression include quantization, which reduces numerical precision, and
                    knowledge distillation, which transfers knowledge from a larger teacher model to a smaller student
                    model. The compression technique we focus on is pruning: systematically removing individual weights,
                    neurons, or entire layers from a neural network by zeroing their values based on an importance
                    criterion.</p>

                <p>
                    Our work is inspired by gradient-based pruning methods that use the gradient of the loss with
                    respect
                    to the weights as part of the pruning criterion. Unlike prior approaches that rely on cross-entropy
                    gradients from language-model pretraining, we instead use gradients from reward model loss functions
                    <sup>
                        <a href="#ref-schulman2017-ppo">[1]</a>
                        <a href="#ref-williams1992-reinforce">[2]</a>
                    </sup>,
                    in the hope of better reflecting alignment with human preferences during pruning. We
                    hypothesize that pruning with reward-specific gradients will preferentially preserve capabilities
                    aligned with that reward signal—for example, a pruned model using a safety-oriented reward model
                    will better maintain performance on truthfulness and toxicity benchmarks compared to pruned models
                    using a reward model trained on general instruction-following data. We test this hypothesis by
                    training multiple reward models from distinct feedback sources and systematically comparing their
                    effects on downstream task performance after pruning.
                </p>


                <!-- Example figure block; replace src and caption when you have a real figure
                <div class="figure-row">
                    <div class="figure-main">
                        <img src="figure1.png" alt="Example plot of pruning performance">
                    </div>
                    <div class="figure-caption">
                        <strong>Figure 1:</strong> Example caption describing the main result of the figure.
                        Update this text and the image source once you have your actual figure.
                    </div>
                </div>
                -->
            </section>

            <section id="background">
                <h2>Background and Related Work</h2>
                <p>
                    <!-- Your background text goes here -->
                    Curabitur ligula sapien, tincidunt non, euismod vitae, posuere
                    imperdiet, leo. Maecenas malesuada.
                </p>
                <p>
                    \[
                    \mathbf{W}_m[i,j] = \alpha \cdot \left|\mathbf{W}[i,j]\right| \cdot \left\|\mathbf{G}[:,
                    i,j]\right\|_{p}
                    + \left|\mathbf{W}[i,j]\right| \cdot \left\| \mathbf{X}[:,j]\right\|_2
                    \]
                </p>
            </section>

            <section id="methods">
                <h2>Methods</h2>
                <p>
                    <!-- Your methods text goes here -->
                    Fusce fermentum odio nec arcu. Vivamus viverra fermentum felis.
                </p>
            </section>

            <section id="results">
                <h2>Results</h2>
                <p>
                    <!-- Your results text goes here -->
                    Pellentesque habitant morbi tristique senectus et netus et malesuada
                    fames ac turpis egestas.
                </p>

                <!-- 7 tasks also used in WANDA and GLP Pruner paper -->
                <table border="1" cellspacing="0" cellpadding="4">
                    <thead>
                        <tr>
                            <th></th>
                            <th>boolq</th>
                            <th>rte</th>
                            <th>hellaswag</th>
                            <th>arc_easy</th>
                            <th>arc_challenge</th>
                            <th>winogrande</th>
                            <th>openbookqa</th>
                            <th>mean</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>No Pruning</td>
                            <td>0.64</td>
                            <td>0.56</td>
                            <td>0.48</td>
                            <td>0.66</td>
                            <td>0.32</td>
                            <td>0.61</td>
                            <td>0.28</td>
                            <td>0.51</td>
                        </tr>
                        <tr>
                            <td>Arena (50%)</td>
                            <td>0.61</td>
                            <td>0.52</td>
                            <td>0.35</td>
                            <td>0.53</td>
                            <td>0.23</td>
                            <td>0.55</td>
                            <td>0.17</td>
                            <td>0.42</td>
                        </tr>
                        <tr>
                            <td>HH (50%)</td>
                            <td>0.61</td>
                            <td>0.53</td>
                            <td>0.30</td>
                            <td>0.42</td>
                            <td>0.20</td>
                            <td>0.52</td>
                            <td>0.12</td>
                            <td>0.39</td>
                        </tr>
                        <tr>
                            <td>Magnitude (50%)</td>
                            <td>0.39</td>
                            <td>0.50</td>
                            <td>0.26</td>
                            <td>0.27</td>
                            <td>0.19</td>
                            <td>0.49</td>
                            <td>0.15</td>
                            <td>0.32</td>
                        </tr>
                        <tr>
                            <td>UF (50%)</td>
                            <td>0.59</td>
                            <td>0.53</td>
                            <td>0.29</td>
                            <td>0.40</td>
                            <td>0.19</td>
                            <td>0.51</td>
                            <td>0.12</td>
                            <td>0.38</td>
                        </tr>
                    </tbody>
                </table>

                <table border="1" cellspacing="0" cellpadding="4">
                    <thead>
                        <tr>
                            <th></th>
                            <th>No Pruning</th>
                            <th>Magnitude (50%)</th>
                            <th>HH (50%)</th>
                            <th>UF (50%)</th>
                            <th>Arena (50%)</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Perplexity</td>
                            <td>9.916</td>
                            <td>1732.262</td>
                            <td>54.618</td>
                            <td>61.442</td>
                            <td>22.561</td>
                        </tr>
                    </tbody>
                </table>

            </section>

            <section id="conclusion">
                <h2>Conclusion</h2>
                <p>
                    <!-- Your conclusion text goes here -->
                    Integer lacinia. Praesent blandit laoreet nibh. Fusce convallis
                    metus id felis luctus adipiscing.
                </p>
            </section>

            <section id="references">
                <h2>References</h2>
                <div class="references">
                    <ol>
                        <!-- [1] PPO -->
                        <li id="ref-schulman2017-ppo">
                            J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov,
                            “Proximal Policy Optimization Algorithms,”
                            <em>arXiv preprint</em> arXiv:1707.06347, 2017.
                            [Online]. Available:
                            <a href="https://arxiv.org/abs/1707.06347" target="_blank" rel="noopener noreferrer">
                                https://arxiv.org/abs/1707.06347
                            </a>
                        </li>

                        <!-- [2] REINFORCE -->
                        <li id="ref-williams1992-reinforce">
                            R. J. Williams,
                            “Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning,”
                            <em>Machine Learning</em>, vol. 8, pp. 229–256, 1992.
                            [Online]. Available:
                            <a href="https://link.springer.com/article/10.1007/BF00992696" target="_blank"
                                rel="noopener noreferrer">
                                https://link.springer.com/article/10.1007/BF00992696
                            </a>
                        </li>

                        <!-- Existing pruning / LLM papers (now [3]–[6]) -->
                        <li id="ref-sun2023-simple">
                            M. Sun, Z. Liu, A. Bair, and J. Z. Kolter,
                            “A Simple and Effective Pruning Approach for Large Language Models,”
                            <em>arXiv preprint</em> arXiv:2306.11695, 2023.
                            [Online]. Available:
                            <a href="https://arxiv.org/abs/2306.11695" target="_blank" rel="noopener noreferrer">
                                https://arxiv.org/abs/2306.11695
                            </a>
                        </li>
                        <li id="ref-das2023-beyond">
                            R. J. Das, M. Sun, L. Ma, and Z. Shen,
                            “Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models,”
                            <em>arXiv preprint</em> arXiv:2311.04902, 2023.
                            [Online]. Available:
                            <a href="https://arxiv.org/abs/2311.04902" target="_blank" rel="noopener noreferrer">
                                https://arxiv.org/abs/2311.04902
                            </a>
                        </li>
                        <li id="ref-ouyang2022-instructgpt">
                            L. Ouyang, J. Wu, X. Jiang, <em>et al.</em>,
                            “Training Language Models to Follow Instructions with Human Feedback,”
                            <em>arXiv preprint</em> arXiv:2203.02155, 2022.
                            [Online]. Available:
                            <a href="https://arxiv.org/abs/2203.02155" target="_blank" rel="noopener noreferrer">
                                https://arxiv.org/abs/2203.02155
                            </a>
                        </li>
                        <li id="ref-lee2018-snip">
                            N. Lee, T. Ajanthan, and P. H. S. Torr,
                            “SNIP: Single-shot Network Pruning Based on Connection Sensitivity,”
                            <em>arXiv preprint</em> arXiv:1810.02340, 2018.
                            [Online]. Available:
                            <a href="https://arxiv.org/abs/1810.02340" target="_blank" rel="noopener noreferrer">
                                https://arxiv.org/abs/1810.02340
                            </a>
                        </li>

                        <!-- lm-eval-harness & benchmarks -->
                        <li id="ref-gao2021-lmeval">
                            L. Gao, J. Tow, S. Biderman, S. Black, A. DiPofi, C. Foster, <em>et al.</em>,
                            “A Framework for Few-shot Language Model Evaluation,” 2021.
                            [Online]. Available:
                            <a href="https://github.com/EleutherAI/lm-evaluation-harness" target="_blank"
                                rel="noopener noreferrer">
                                https://github.com/EleutherAI/lm-evaluation-harness
                            </a>
                        </li>
                        <li id="ref-clark2019-boolq">
                            C. Clark, K. Lee, M.-W. Chang, T. Kwiatkowski, M. Collins, and K. Toutanova,
                            “BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions,”
                            in <em>Proc. NAACL-HLT</em>, 2019.
                            [Online]. Available:
                            <a href="https://arxiv.org/abs/1905.10044" target="_blank" rel="noopener noreferrer">
                                https://arxiv.org/abs/1905.10044
                            </a>
                        </li>
                        <li id="ref-wang2018-glue">
                            A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman,
                            “GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding,”
                            <em>arXiv preprint</em> arXiv:1804.07461, 2018.
                            [Online]. Available:
                            <a href="https://arxiv.org/abs/1804.07461" target="_blank" rel="noopener noreferrer">
                                https://arxiv.org/abs/1804.07461
                            </a>
                        </li>
                        <li id="ref-zellers2019-hellaswag">
                            R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi,
                            “HellaSwag: Can a Machine Really Finish Your Sentence?,”
                            in <em>Proc. ACL</em>, 2019.
                            [Online]. Available:
                            <a href="https://arxiv.org/abs/1905.07830" target="_blank" rel="noopener noreferrer">
                                https://arxiv.org/abs/1905.07830
                            </a>
                        </li>
                        <li id="ref-sakaguchi2019-winogrande">
                            K. Sakaguchi, R. Bras, C. Bhagavatula, and Y. Choi,
                            “WinoGrande: An Adversarial Winograd Schema Challenge at Scale,”
                            <em>arXiv preprint</em> arXiv:1907.10641, 2019.
                            [Online]. Available:
                            <a href="https://arxiv.org/abs/1907.10641" target="_blank" rel="noopener noreferrer">
                                https://arxiv.org/abs/1907.10641
                            </a>
                        </li>
                        <li id="ref-clark2018-arc">
                            P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord,
                            “Think You Have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge,”
                            <em>arXiv preprint</em> arXiv:1803.05457, 2018.
                            [Online]. Available:
                            <a href="https://arxiv.org/abs/1803.05457" target="_blank" rel="noopener noreferrer">
                                https://arxiv.org/abs/1803.05457
                            </a>
                        </li>
                        <li id="ref-mihaylov2018-openbookqa">
                            T. Mihaylov, P. Clark, T. Khot, and A. Sabharwal,
                            “Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering,”
                            in <em>Proc. EMNLP</em>, 2018.
                            [Online]. Available:
                            <a href="https://arxiv.org/abs/1809.02789" target="_blank" rel="noopener noreferrer">
                                https://arxiv.org/abs/1809.02789
                            </a>
                        </li>

                        <!-- Compression baselines -->
                        <li id="ref-dettmers2022-llmint8">
                            T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer,
                            “LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale,”
                            <em>arXiv preprint</em> arXiv:2208.07339, 2022.
                            [Online]. Available:
                            <a href="https://arxiv.org/abs/2208.07339" target="_blank" rel="noopener noreferrer">
                                https://arxiv.org/abs/2208.07339
                            </a>
                        </li>
                        <li id="ref-frantar2023-sparsegpt">
                            E. Frantar and D. Alistarh,
                            “SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot,”
                            in <em>Proc. ICML</em>, 2023.
                            [Online]. Available:
                            <a href="https://arxiv.org/abs/2301.00774" target="_blank" rel="noopener noreferrer">
                                https://arxiv.org/abs/2301.00774
                            </a>
                        </li>
                    </ol>
                </div>
            </section>

        </article>
    </div>

</body>

</html>